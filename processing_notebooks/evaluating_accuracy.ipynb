{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluating Accuracy.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"98d6938104b94892952c4b58b0c2dd3e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_098ec57a37674238af39fdf98c6e0b4d","IPY_MODEL_fc95a68d63f34ce0af5b80396b307a85","IPY_MODEL_097134abac9b434ab6a81f578cdc8c08"],"layout":"IPY_MODEL_79153c65b9724dc59d95c1869bb08502"}},"098ec57a37674238af39fdf98c6e0b4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1b985e2e7784d598434ee3a24caa039","placeholder":"​","style":"IPY_MODEL_1a5add8e2f434bc8993244a6c7eb159e","value":"Downloading: 100%"}},"fc95a68d63f34ce0af5b80396b307a85":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_242839c5638049a1bfbee1db3cd4b296","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53e7f80bfc9d47068b4f4e414bb29e6e","value":898823}},"097134abac9b434ab6a81f578cdc8c08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbe5a2c7d2c943c3b485e07b449dadc2","placeholder":"​","style":"IPY_MODEL_d18694549c32408197e65d0eea4bbba8","value":" 878k/878k [00:00&lt;00:00, 1.50MB/s]"}},"79153c65b9724dc59d95c1869bb08502":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1b985e2e7784d598434ee3a24caa039":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a5add8e2f434bc8993244a6c7eb159e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"242839c5638049a1bfbee1db3cd4b296":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53e7f80bfc9d47068b4f4e414bb29e6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbe5a2c7d2c943c3b485e07b449dadc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d18694549c32408197e65d0eea4bbba8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd601eed29514d70898fb88bb0b553ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c75532aebc2249d28f881cb40de2b1bd","IPY_MODEL_a3b7e7b668e64d59a7b1e522c5934385","IPY_MODEL_a8744634cc99466fb70bdf50dc47180d"],"layout":"IPY_MODEL_3e0c86772d904412a7e9c3c03d990f2a"}},"c75532aebc2249d28f881cb40de2b1bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38d703e28ac24d69bc6958924a037c4d","placeholder":"​","style":"IPY_MODEL_995896f03d6244c3b7779f10f0a82c1e","value":"Downloading: 100%"}},"a3b7e7b668e64d59a7b1e522c5934385":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64630e1b5c8f4213a1f00a6a1fb084ac","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd5bbaacd7dd4753849b5f3941a9d6de","value":456318}},"a8744634cc99466fb70bdf50dc47180d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_654a59fae9a34ecabd71497053dd930c","placeholder":"​","style":"IPY_MODEL_a919cada57864d8b9382d7cf84ec172b","value":" 446k/446k [00:00&lt;00:00, 1.56MB/s]"}},"3e0c86772d904412a7e9c3c03d990f2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38d703e28ac24d69bc6958924a037c4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"995896f03d6244c3b7779f10f0a82c1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64630e1b5c8f4213a1f00a6a1fb084ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd5bbaacd7dd4753849b5f3941a9d6de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"654a59fae9a34ecabd71497053dd930c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a919cada57864d8b9382d7cf84ec172b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8395367c85f749eeb83ac30b3d9327e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c8fd55aedf040fd963f96fac52cd9cd","IPY_MODEL_0f71d37a43964230ad9ed655f380fcc4","IPY_MODEL_6ec53c5b2aab4b6d9ec532ef5b601f8c"],"layout":"IPY_MODEL_c32055e8d08c400aa0fdbe47e1048181"}},"9c8fd55aedf040fd963f96fac52cd9cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89fc70e26cf142eba65662164659104d","placeholder":"​","style":"IPY_MODEL_242f239e3aef4eb5bbbccd056e17ce54","value":"Downloading: 100%"}},"0f71d37a43964230ad9ed655f380fcc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52debd2619844ff5acbacd460044002c","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85e3e3abaf604eb58d6ea266a5718906","value":481}},"6ec53c5b2aab4b6d9ec532ef5b601f8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_897fb5534f6141c1b5c6bc6bfaaa0464","placeholder":"​","style":"IPY_MODEL_ad61b83a41054434b7ab2c6e92224aed","value":" 481/481 [00:00&lt;00:00, 13.2kB/s]"}},"c32055e8d08c400aa0fdbe47e1048181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89fc70e26cf142eba65662164659104d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"242f239e3aef4eb5bbbccd056e17ce54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52debd2619844ff5acbacd460044002c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85e3e3abaf604eb58d6ea266a5718906":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"897fb5534f6141c1b5c6bc6bfaaa0464":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad61b83a41054434b7ab2c6e92224aed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54741b56991644859989ca4a07308bf6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e402a046a51410d8a8bbdccf2b02743","IPY_MODEL_dead4a9e403d4ed29327ffc51ac96c20","IPY_MODEL_a6592c2027a740939b5e3bff49f3ac87"],"layout":"IPY_MODEL_d36c41532a424e28b182d448da94a9de"}},"7e402a046a51410d8a8bbdccf2b02743":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_caa519b01a304027875263e6b99faca0","placeholder":"​","style":"IPY_MODEL_980fadd76fe44dce85350b540b513f2d","value":"Downloading: 100%"}},"dead4a9e403d4ed29327ffc51ac96c20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b32f6de342674872abd19f4baf565acb","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79514c639b6b4708a2e5294b918d9667","value":501200538}},"a6592c2027a740939b5e3bff49f3ac87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd96dc566c31424f9ab2ee40de57f4ef","placeholder":"​","style":"IPY_MODEL_efaf4caef1c84059af8998dc7468c206","value":" 478M/478M [00:21&lt;00:00, 31.7MB/s]"}},"d36c41532a424e28b182d448da94a9de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caa519b01a304027875263e6b99faca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"980fadd76fe44dce85350b540b513f2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b32f6de342674872abd19f4baf565acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79514c639b6b4708a2e5294b918d9667":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd96dc566c31424f9ab2ee40de57f4ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efaf4caef1c84059af8998dc7468c206":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0mb28Sr3vOni","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655077089394,"user_tz":420,"elapsed":3619,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"b946da62-3c6f-4f5a-a9c1-4a14e8b7c7b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["%cd gdrive/MyDrive/LibKGE Test/kge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PkOrDbOv3vz","executionInfo":{"status":"ok","timestamp":1655077095047,"user_tz":420,"elapsed":343,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"26c62fdc-1fc0-4488-8e44-cd6dcfb81092"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/12lK9g6Ccl-njCvAuL28xCjV1XOyHp9bU/LibKGE Test/kge\n"]}]},{"cell_type":"code","source":["! pip install transformers\n","! pip install path\n","! pip install rapidfuzz\n","! pip install sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLzQYcXlwCnH","executionInfo":{"status":"ok","timestamp":1655077108868,"user_tz":420,"elapsed":11974,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"b7634b1a-9184-4892-bc00-de4276d8beb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: path in /usr/local/lib/python3.7/dist-packages (16.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.7/dist-packages (2.0.11)\n","Requirement already satisfied: jarowinkler<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from rapidfuzz) (1.0.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.7.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.19.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"]}]},{"cell_type":"code","source":["import torch\n","from kge.model.kge_model import KgeModel\n","from kge.util.io import load_checkpoint\n","\n","import spacy\n","\n","from sentence_transformers import SentenceTransformer, util\n","import numpy as np"],"metadata":{"id":"8Jp8OzeSwgcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# returns entity embeddings \n","def getEntityEmbeddings(kge_model, entity_dict):\n","    e = {}\n","    embedder = kge_model._entity_embedder\n","    f = open(entity_dict, 'r')\n","    for line in f:\n","        line = line[:-1].split('\\t')\n","        ent_id = int(line[0])\n","        ent_name = line[1]\n","        e[ent_name] = embedder._embeddings(torch.LongTensor([ent_id]))[0]\n","    f.close()\n","    return e"],"metadata":{"id":"PiiCppRwwh45"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creates dictionaries that allow us to access entity name once we get the answer\n","def prepare_embeddings(embedding_dict):\n","    entity2idx = {}\n","    idx2entity = {}\n","    i = 0\n","    embedding_matrix = []\n","    for key, entity in embedding_dict.items():\n","        entity2idx[key] = i\n","        idx2entity[i] = key\n","        i += 1\n","        embedding_matrix.append(entity)\n","    return entity2idx, idx2entity, embedding_matrix"],"metadata":{"id":"f12biAXgwico"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# given a head entity\n","# return a dictionary with the highest scoring entities as keys and scores as values\n","def get_answer(question, head_entity, k_answers, device, dataloader, model, entity2idx, idx2entity):\n","\n","      scores_dict = dict()\n","\n","      question_tokenized, attention_mask = dataloader.tokenize_question(question)\n","      head = torch.tensor(entity2idx[head_entity], dtype = torch.long).to(device) \n","      question_tokenized = question_tokenized.to(device)\n","      attention_mask = attention_mask.to(device)\n","      #scores = model.get_score_ranked(head=head, question_tokenized=question_tokenized, attention_mask=attention_mask)[0] #half model\n","      scores = model.get_score_ranked(head=head, question_tokenized=question_tokenized, attention_mask=attention_mask)[0] #for full model\n","      mask = torch.zeros(len(entity2idx)).to(device)\n","      mask[head] = 1\n","      #reduce scores of all non-candidates\n","      new_scores = scores - (mask*99999)\n","      new_scores = torch.sigmoid(new_scores)\n","\n","      # add scores for each head entity to a dictionary\n","      for i in range(len(new_scores)):\n","        scores_dict[new_scores[i].item()] = i\n","\n","      answers_dict = dict()\n","\n","      # take the highest scoring answer entities and scores from the dictionary\n","      for answer in range(k_answers):\n","        highest_score = max(scores_dict.keys())\n","        max_answer_idx = scores_dict[highest_score]\n","        answers_dict[idx2entity[max_answer_idx]] = highest_score\n","        del scores_dict[highest_score]\n","\n","      return answers_dict"],"metadata":{"id":"hUYZepVrdLXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from collections import defaultdict\n","from transformers import RobertaTokenizer\n","\n","# process questions\n","class DatasetMetaQA(Dataset):\n","    def __init__(self, data, entities, entity2idx):\n","        self.data = data\n","        self.entities = entities\n","        self.entity2idx = entity2idx\n","        self.pos_dict = defaultdict(list)\n","        self.neg_dict = defaultdict(list)\n","        self.index_array = list(self.entities.keys())\n","        self.tokenizer_class = RobertaTokenizer\n","        self.pretrained_weights = 'roberta-base'\n","        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def pad_sequence(self, arr, max_len=128):\n","        num_to_add = max_len - len(arr)\n","        for _ in range(num_to_add):\n","            arr.append('<pad>')\n","        return arr\n","\n","    def toOneHot(self, indices):\n","        indices = torch.LongTensor(indices)\n","        batch_size = len(indices)\n","        vec_len = len(self.entity2idx)\n","        one_hot = torch.FloatTensor(vec_len)\n","        one_hot.zero_()\n","        # one_hot = -torch.ones(vec_len, dtype=torch.float32)\n","        one_hot.scatter_(0, indices, 1)\n","        return one_hot\n","\n","    def __getitem__(self, index):\n","        data_point = self.data[index]\n","        question_text = data_point[1]\n","        question_tokenized, attention_mask = self.tokenize_question(question_text)\n","        head_id = self.entity2idx[data_point[0].strip()]\n","        tail_ids = []\n","        for tail_name in data_point[2]:\n","            tail_name = tail_name.strip()\n","            #TODO: dunno if this is right way of doing things\n","            if tail_name in self.entity2idx:\n","                tail_ids.append(self.entity2idx[tail_name])\n","        tail_onehot = self.toOneHot(tail_ids)\n","        return question_tokenized, attention_mask, head_id, tail_onehot \n","\n","    def tokenize_question(self, question):\n","        question = \"<s> \" + question + \" </s>\"\n","        question_tokenized = self.tokenizer.tokenize(question)\n","        question_tokenized = self.pad_sequence(question_tokenized, 64)\n","        question_tokenized = torch.tensor(self.tokenizer.encode(question_tokenized, add_special_tokens=False))\n","        attention_mask = []\n","        for q in question_tokenized:\n","            # 1 means padding token\n","            if q == 1:\n","                attention_mask.append(0)\n","            else:\n","                attention_mask.append(1)\n","        return question_tokenized, torch.tensor(attention_mask, dtype=torch.long)\n","\n","class DataLoaderMetaQA(DataLoader):\n","    def __init__(self, *args, **kwargs):\n","        super(DataLoaderMetaQA, self).__init__(*args, **kwargs)\n","        self.collate_fn = _collate_fn"],"metadata":{"id":"lQT-N7RkwnUw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.utils\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.nn.init import xavier_normal_\n","from transformers import RobertaModel\n","import random\n","\n","# contains question embedding process and score ranking functions\n","class RelationExtractor(nn.Module):\n","    def __init__(self, embedding_dim, relation_dim, num_entities, pretrained_embeddings, device,\n","    entdrop=0.0, reldrop=0.0, scoredrop=0.0, l3_reg=0.0, model=\"ComplEx\", ls=0.0, do_batch_norm=True, freeze=True):\n","        super(RelationExtractor, self).__init__()\n","        self.device = device\n","        self.model = model\n","        #self.model_attr_accessor = model.module\n","        self.freeze = freeze\n","        self.label_smoothing = ls\n","        self.l3_reg = l3_reg\n","        self.do_batch_norm = do_batch_norm\n","        if not self.do_batch_norm:\n","            print(\"Not doing batch norm\")\n","        self.roberta_pretrained_weights = \"roberta-base\"\n","        self.roberta_model = RobertaModel.from_pretrained(self.roberta_pretrained_weights)\n","        for param in self.roberta_model.parameters():\n","            param.requires_grad = True\n","        if self.model == \"ComplEx\":\n","            multiplier = 2\n","            self.getScores = self.ComplEx\n","        else:\n","            print(\"Incorrect model specified:\", self.model)\n","            exit(0)\n","        print(\"Model is\", self.model)\n","        self.hidden_dim = 768\n","        self.relation_dim = relation_dim * multiplier\n","        self.num_entities = num_entities\n","        self.loss = self.kge_loss\n","        self.rel_dropout = torch.nn.Dropout(reldrop)\n","        self.ent_dropout = torch.nn.Dropout(entdrop)\n","        self.score_dropout = torch.nn.Dropout(scoredrop)\n","        self.fcnn_dropout = torch.nn.Dropout(0.1)\n","        print(\"Frozen:\", self.freeze)\n","        self.embedding = nn.Embedding.from_pretrained(torch.stack(pretrained_embeddings, dim=0), freeze=self.freeze)\n","        print(self.embedding.weight.shape)\n","        self.mid1 = 512\n","        self.mid2 = 512\n","        self.mid3 = 512\n","        self.mid4 = 512\n","        self.lin1 = nn.Linear(self.hidden_dim, self.mid1)\n","        self.lin2 = nn.Linear(self.mid1, self.mid2)\n","        self.lin3 = nn.Linear(self.mid2, self.mid3)\n","        self.lin4 = nn.Linear(self.mid3, self.mid4)\n","        self.hidden2rel = nn.Linear(self.mid4, self.relation_dim)\n","        self.bn0 = torch.nn.BatchNorm1d(multiplier)\n","        self.bn2 = torch.nn.BatchNorm1d(multiplier)\n","        self.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n","        self._klloss = torch.nn.KLDivLoss(reduction=\"sum\")\n","\n","    def set_bn_eval(self):\n","        self.bn0.eval()\n","        self.bn2.eval()\n","\n","    def kge_loss(self, scores, targets):\n","        # loss = torch.mean(scores*targets)\n","        return self._klloss(\n","            F.log_softmax(scores, dim=1), F.normalize(targets.float(), p=1, dim=1)\n","        )\n","\n","    def ComplEx(self, head, relation):\n","        head = torch.stack(list(torch.chunk(head, 2, dim=1)), dim=1)\n","        if self.do_batch_norm:\n","            head = self.bn0(head)\n","        head = self.ent_dropout(head)\n","        relation = self.rel_dropout(relation)\n","        head = head.permute(1, 0, 2)\n","        re_head = head[0]\n","        im_head = head[1]\n","        re_relation, im_relation = torch.chunk(relation, 2, dim=1)\n","        re_tail, im_tail = torch.chunk(self.embedding.weight, 2, dim =1)\n","        re_score = re_head * re_relation - im_head * im_relation\n","        im_score = re_head * im_relation + im_head * re_relation\n","        score = torch.stack([re_score, im_score], dim=1)\n","        if self.do_batch_norm:\n","            score = self.bn2(score)\n","        score = self.score_dropout(score)\n","        score = score.permute(1, 0, 2)\n","        re_score = score[0]\n","        im_score = score[1]\n","        score = torch.mm(re_score, re_tail.transpose(1,0)) + torch.mm(im_score, im_tail.transpose(1,0))\n","        pred = score\n","        return pred\n","\n","    # Inputs a tokenized question and attention mask, returns a question embedding\n","    def getQuestionEmbedding(self, question_tokenized, attention_mask):\n","        roberta_last_hidden_states = self.roberta_model(question_tokenized, attention_mask=attention_mask)[0]\n","        states = roberta_last_hidden_states.transpose(1,0)\n","        cls_embedding = states[0]\n","        question_embedding = cls_embedding\n","        # question_embedding = torch.mean(roberta_last_hidden_states, dim=1)\n","        return question_embedding\n","\n","    # Forward pass\n","    def forward(self, question_tokenized, attention_mask, p_head, p_tail):\n","        # create question embedding with roBERTa, 768 dim\n","        question_embedding = self.getQuestionEmbedding(question_tokenized, attention_mask)\n","        # NN that sends question_embedding to a vector with dimension = relation_dim\n","        rel_embedding = self.applyNonLinear(question_embedding)\n","        # embedding of the entity in the question\n","        p_head = self.embedding(p_head)\n","        # phi(e_h, e_r, e_a)\n","        pred = self.getScores(p_head, rel_embedding)\n","        actual = p_tail\n","        if self.label_smoothing:\n","            actual = ((1.0-self.label_smoothing)*actual) + (1.0/actual.size(1))\n","        loss = self.loss(pred, actual)\n","        if not self.freeze:\n","            if self.l3_reg:\n","                norm = torch.norm(self.embedding.weight, p=3, dim=-1)\n","                loss = loss + self.l3_reg * torch.sum(norm)\n","        return loss\n","\n","    # modified code based on what it should be from the paper\n","    # pass through a 4 layer neural network\n","    def applyNonLinear(self, outputs):\n","        # linear layer with dropout\n","        outputs = self.fcnn_dropout(self.lin1(outputs))\n","        # reLU activation layer\n","        outputs = F.relu(outputs)\n","        # linear layer with dropout\n","        outputs = self.fcnn_dropout(self.lin2(outputs))\n","        # reLU activation layer\n","        outputs = F.relu(outputs)\n","        # linear layer\n","        outputs = self.lin3(outputs)\n","        # reLU activation layer\n","        outputs = F.relu(outputs)\n","        # linear layer\n","        outputs = self.lin4(outputs)\n","        # reLU activation layer\n","        outputs = F.relu(outputs)\n","        # linear layer\n","        outputs = self.hidden2rel(outputs)\n","        return outputs\n","\n","    def get_score_ranked(self, head, question_tokenized, attention_mask):\n","        question_embedding = self.getQuestionEmbedding(question_tokenized.unsqueeze(0), attention_mask.unsqueeze(0))\n","        rel_embedding = self.applyNonLinear(question_embedding)\n","        head = self.embedding(head).unsqueeze(0)\n","        scores = self.getScores(head, rel_embedding)\n","        return scores"],"metadata":{"id":"o50omeq7wyp7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the half dataset:"],"metadata":{"id":"taeZw7ZuT-y8"}},{"cell_type":"code","source":["# best entity embedding weights trained\n","checkpoint = load_checkpoint('checkpoint_best.pt')\n","model = KgeModel.create_from(checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L7itEI4Bw7Ge","executionInfo":{"status":"ok","timestamp":1655077155276,"user_tz":420,"elapsed":2440,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"9562fcb8-b245-4c90-fe81-bc7bedfbe06f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading configuration of dataset fb_natural_language_data_half from /content/gdrive/.shortcut-targets-by-id/12lK9g6Ccl-njCvAuL28xCjV1XOyHp9bU/LibKGE Test/kge/data/fb_natural_language_data_half ...\n"]},{"output_type":"stream","name":"stderr","text":["Warning: Avoided overwrite of already set option dataset.name. Used fb_natural_language_data_half instead of /content/gdrive/.shortcut-targets-by-id/12lK9g6Ccl-njCvAuL28xCjV1XOyHp9bU/LibKGE Test/kge/data/half_fb_natural_language_data.\n"]}]},{"cell_type":"code","source":["#getting the entity embeddings for the half dataset\n","e = getEntityEmbeddings(model, \"data/fb_natural_language_data_half/entity_ids.del\")"],"metadata":{"id":"z0h5ilJRxnif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entity2idx, idx2entity, embedding_matrix = prepare_embeddings(e)"],"metadata":{"id":"SUiSrVA8xwOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(entity2idx), len(idx2entity)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Jzw4WzNXaUW","executionInfo":{"status":"ok","timestamp":1655077196763,"user_tz":420,"elapsed":39,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"8ee207be-2514-46a5-c384-eed338af413e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(479812, 479812)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["from transformers import RobertaModel\n","device = torch.device(\"cpu\")\n","\n","#initialize model with question embedding and score ranking functions that does answer selection\n","model2 = RelationExtractor(embedding_dim=256, num_entities = len(idx2entity), relation_dim=50, \n","                              pretrained_embeddings=embedding_matrix, device=device)\n","\n","#load in the trained half model \n","fname = \"best_score_model.pt\"\n","\n","# fit model\n","model2.load_state_dict(torch.load(fname, map_location=torch.device('cpu')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lyRtOog4x2s0","executionInfo":{"status":"ok","timestamp":1655077205041,"user_tz":420,"elapsed":8308,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"6e330ac3-8a2e-4538-dc11-a6efae6a2558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Model is ComplEx\n","Frozen: True\n","torch.Size([479812, 100])\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["For the full dataset:"],"metadata":{"id":"W6oajpquUDkU"}},{"cell_type":"code","source":["checkpoint_full = load_checkpoint('new_checkpoint_best.pt')\n"],"metadata":{"id":"lFmDHMmJQWOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_model = KgeModel.create_from(checkpoint_full)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y0wMwuCnU-3X","executionInfo":{"status":"ok","timestamp":1655076427593,"user_tz":420,"elapsed":3720,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"7ab44f9b-82a0-45f4-a2f4-ab23b655b121"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading configuration of dataset fb_natural_language_data_full from /content/gdrive/.shortcut-targets-by-id/12lK9g6Ccl-njCvAuL28xCjV1XOyHp9bU/LibKGE Test/kge/data/fb_natural_language_data_full ...\n"]},{"output_type":"stream","name":"stderr","text":["Warning: Avoided overwrite of already set option dataset.name. Used fb_natural_language_data_full instead of /content/gdrive/.shortcut-targets-by-id/12lK9g6Ccl-njCvAuL28xCjV1XOyHp9bU/LibKGE Test/kge/data/preprocess/fb_natural_language_data.\n"]}]},{"cell_type":"code","source":["e_full = getEntityEmbeddings(full_model, \"data/fb_natural_language_data_full/entity_ids.del\")\n"],"metadata":{"id":"A3NWV1RGUC1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entity2idx_full, idx2entity_full, embedding_matrix_full = prepare_embeddings(e_full)"],"metadata":{"id":"sYr1w_AMUYCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(entity2idx_full), len(idx2entity_full)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6C2WmP7XER4","executionInfo":{"status":"ok","timestamp":1655076443719,"user_tz":420,"elapsed":58,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"605f75ce-a0d4-418c-9938-e56260ccaf79"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(701166, 701166)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["\n","from transformers import RobertaModel\n","device = torch.device(\"cpu\")\n","\n","#initialize model with question embedding and score ranking functions that does answer selection\n","model2_full = RelationExtractor(embedding_dim=256, num_entities = len(idx2entity_full), relation_dim=50, \n","                              pretrained_embeddings=embedding_matrix_full, device=device)\n","#model2_full = nn.DataParallel(model2_full)\n","fname = \"retrained_best_score_model.pt\"\n","\n","# fit model\n","model2_full.load_state_dict(torch.load(fname, map_location=torch.device('cpu')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["8395367c85f749eeb83ac30b3d9327e0","9c8fd55aedf040fd963f96fac52cd9cd","0f71d37a43964230ad9ed655f380fcc4","6ec53c5b2aab4b6d9ec532ef5b601f8c","c32055e8d08c400aa0fdbe47e1048181","89fc70e26cf142eba65662164659104d","242f239e3aef4eb5bbbccd056e17ce54","52debd2619844ff5acbacd460044002c","85e3e3abaf604eb58d6ea266a5718906","897fb5534f6141c1b5c6bc6bfaaa0464","ad61b83a41054434b7ab2c6e92224aed","54741b56991644859989ca4a07308bf6","7e402a046a51410d8a8bbdccf2b02743","dead4a9e403d4ed29327ffc51ac96c20","a6592c2027a740939b5e3bff49f3ac87","d36c41532a424e28b182d448da94a9de","caa519b01a304027875263e6b99faca0","980fadd76fe44dce85350b540b513f2d","b32f6de342674872abd19f4baf565acb","79514c639b6b4708a2e5294b918d9667","dd96dc566c31424f9ab2ee40de57f4ef","efaf4caef1c84059af8998dc7468c206"]},"id":"49-IJiIsUa4M","executionInfo":{"status":"ok","timestamp":1655076475791,"user_tz":420,"elapsed":32087,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"191b51d5-c16a-4a2e-f573-522d39e838a6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8395367c85f749eeb83ac30b3d9327e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54741b56991644859989ca4a07308bf6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Model is ComplEx\n","Frozen: True\n","torch.Size([701166, 100])\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Evaluating Accuracy Starts Here: Given the correct head entity, what is the accuracy for getting the right answer with the natural language question?"],"metadata":{"id":"IXLdBBM0a0cj"}},{"cell_type":"code","source":["#load in test data \n","with open('qa_test_nl.txt') as f:  \n","  qa_test_lines = f.readlines()\n","  "],"metadata":{"id":"Apodiozua4-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate the accuracy given the test data in the form of (natural language question [head entity] answer 1|answer 2|answer ...), the entity embeddings, the RelationExtractor model, entity2idx, idx2entity."],"metadata":{"id":"jXVY8IG3042o"}},{"cell_type":"code","source":["\n","def accuracy(test_data, e, model2, entity2idx, idx2entity):\n","  correct_count = 0\n","  for line in test_data:\n","    question = line.split(' [')[0]\n","    given_head_entity = line.split('[', 1)[1].split(']')[0]\n","    correct_answers = line.split(']')[-1]\n","    correct_answers = correct_answers.replace('\\t','').replace('\\n','')\n","    correct_answers = correct_answers.split(\"|\")\n","    dataloader = DatasetMetaQA(question, e, entity2idx)\n","    output_answers = list(get_answer(question, given_head_entity, 1, device, dataloader, model2, entity2idx, idx2entity))[0]\n","    if output_answers in correct_answers:\n","      correct_count += 1\n","\n","  return correct_count/len(test_data)\n","    "],"metadata":{"id":"u0MvIUnZbAoI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#accuracy on the full model\n","accuracy(qa_test_lines, e_full, model2_full, entity2idx_full, idx2entity_full)"],"metadata":{"id":"xu9nJtG6kgtI","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["98d6938104b94892952c4b58b0c2dd3e","098ec57a37674238af39fdf98c6e0b4d","fc95a68d63f34ce0af5b80396b307a85","097134abac9b434ab6a81f578cdc8c08","79153c65b9724dc59d95c1869bb08502","b1b985e2e7784d598434ee3a24caa039","1a5add8e2f434bc8993244a6c7eb159e","242839c5638049a1bfbee1db3cd4b296","53e7f80bfc9d47068b4f4e414bb29e6e","cbe5a2c7d2c943c3b485e07b449dadc2","d18694549c32408197e65d0eea4bbba8","fd601eed29514d70898fb88bb0b553ca","c75532aebc2249d28f881cb40de2b1bd","a3b7e7b668e64d59a7b1e522c5934385","a8744634cc99466fb70bdf50dc47180d","3e0c86772d904412a7e9c3c03d990f2a","38d703e28ac24d69bc6958924a037c4d","995896f03d6244c3b7779f10f0a82c1e","64630e1b5c8f4213a1f00a6a1fb084ac","fd5bbaacd7dd4753849b5f3941a9d6de","654a59fae9a34ecabd71497053dd930c","a919cada57864d8b9382d7cf84ec172b"]},"executionInfo":{"status":"ok","timestamp":1655014737995,"user_tz":420,"elapsed":501185,"user":{"displayName":"Nhi Nguyen","userId":"08819446501956192949"}},"outputId":"4143066d-e757-4eec-ab1d-353d15bfbc10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d6938104b94892952c4b58b0c2dd3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd601eed29514d70898fb88bb0b553ca"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1\n","correct count so far 1\n","2\n","3\n","correct count so far 2\n","4\n","correct count so far 3\n","5\n","correct count so far 4\n","6\n","7\n","correct count so far 5\n","8\n","correct count so far 6\n","9\n","correct count so far 7\n","10\n","correct count so far 8\n","11\n","correct count so far 9\n","12\n","13\n","correct count so far 10\n","14\n","correct count so far 11\n","15\n","16\n","17\n","18\n","correct count so far 12\n","19\n","correct count so far 13\n","20\n","correct count so far 14\n","21\n","correct count so far 15\n","22\n","23\n","correct count so far 16\n","24\n","correct count so far 17\n","25\n","26\n","27\n","correct count so far 18\n","28\n","29\n","correct count so far 19\n","30\n","correct count so far 20\n","31\n","32\n","correct count so far 21\n","33\n","correct count so far 22\n","34\n","35\n","36\n","correct count so far 23\n","37\n","38\n","correct count so far 24\n","39\n","correct count so far 25\n","40\n","correct count so far 26\n","41\n","correct count so far 27\n","42\n","correct count so far 28\n","43\n","correct count so far 29\n","44\n","45\n","46\n","47\n","correct count so far 30\n","48\n","49\n","correct count so far 31\n","50\n","51\n","correct count so far 32\n","52\n","53\n","correct count so far 33\n","54\n","correct count so far 34\n","55\n","56\n","correct count so far 35\n","57\n","correct count so far 36\n","58\n","correct count so far 37\n","59\n","correct count so far 38\n","60\n","correct count so far 39\n","61\n","correct count so far 40\n","62\n","63\n","64\n","correct count so far 41\n","65\n","66\n","correct count so far 42\n","67\n","68\n","correct count so far 43\n","69\n","70\n","correct count so far 44\n","71\n","72\n","correct count so far 45\n","73\n","74\n","75\n","correct count so far 46\n","76\n","77\n","correct count so far 47\n","78\n","correct count so far 48\n","79\n","correct count so far 49\n","80\n","correct count so far 50\n","81\n","82\n","83\n","84\n","correct count so far 51\n","85\n","86\n","correct count so far 52\n","87\n","88\n","correct count so far 53\n","89\n","90\n","correct count so far 54\n","91\n","92\n","93\n","correct count so far 55\n","94\n","95\n","96\n","correct count so far 56\n","97\n","98\n","correct count so far 57\n","99\n","correct count so far 58\n","100\n","correct count so far 59\n","101\n","correct count so far 60\n","102\n","correct count so far 61\n","103\n","correct count so far 62\n","104\n","correct count so far 63\n","105\n","106\n","correct count so far 64\n","107\n","108\n","correct count so far 65\n","109\n","correct count so far 66\n","110\n","correct count so far 67\n","111\n","correct count so far 68\n","112\n","correct count so far 69\n","113\n","correct count so far 70\n","114\n","correct count so far 71\n","115\n","correct count so far 72\n","116\n","correct count so far 73\n","117\n","correct count so far 74\n","118\n","119\n","correct count so far 75\n","120\n","correct count so far 76\n","121\n","correct count so far 77\n","122\n","correct count so far 78\n","123\n","correct count so far 79\n","124\n","125\n","correct count so far 80\n","126\n","127\n","correct count so far 81\n","128\n","correct count so far 82\n","129\n","correct count so far 83\n","130\n","correct count so far 84\n","131\n","correct count so far 85\n","132\n","133\n","correct count so far 86\n","134\n","135\n","correct count so far 87\n","136\n","correct count so far 88\n","137\n","correct count so far 89\n","138\n","139\n","correct count so far 90\n","140\n","141\n","correct count so far 91\n","142\n","correct count so far 92\n","143\n","144\n","145\n","correct count so far 93\n"]},{"output_type":"execute_result","data":{"text/plain":["0.636986301369863"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["#accuracy on the half model\n","accuracy(qa_test_lines, e, model2, entity2idx, idx2entity)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91DgGNr0nFSP","executionInfo":{"status":"ok","timestamp":1655015220644,"user_tz":420,"elapsed":372602,"user":{"displayName":"Nhi Nguyen","userId":"08819446501956192949"}},"outputId":"3c27e3ce-44b5-4741-dc36-dee3a2c9a123"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","correct count so far 1\n","1\n","2\n","correct count so far 2\n","3\n","4\n","5\n","correct count so far 3\n","6\n","7\n","correct count so far 4\n","8\n","correct count so far 5\n","9\n","10\n","11\n","correct count so far 6\n","12\n","13\n","14\n","correct count so far 7\n","15\n","16\n","17\n","18\n","correct count so far 8\n","19\n","20\n","correct count so far 9\n","21\n","correct count so far 10\n","22\n","23\n","correct count so far 11\n","24\n","25\n","26\n","27\n","correct count so far 12\n","28\n","correct count so far 13\n","29\n","correct count so far 14\n","30\n","31\n","32\n","correct count so far 15\n","33\n","34\n","correct count so far 16\n","35\n","36\n","37\n","38\n","correct count so far 17\n","39\n","correct count so far 18\n","40\n","41\n","42\n","correct count so far 19\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","correct count so far 20\n","52\n","53\n","54\n","55\n","56\n","correct count so far 21\n","57\n","58\n","59\n","correct count so far 22\n","60\n","correct count so far 23\n","61\n","62\n","63\n","64\n","65\n","66\n","correct count so far 24\n","67\n","68\n","69\n","70\n","71\n","72\n","correct count so far 25\n","73\n","74\n","75\n","76\n","77\n","correct count so far 26\n","78\n","correct count so far 27\n","79\n","correct count so far 28\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","correct count so far 29\n","87\n","correct count so far 30\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","correct count so far 31\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","correct count so far 32\n","107\n","108\n","correct count so far 33\n","109\n","correct count so far 34\n","110\n","111\n","112\n","correct count so far 35\n","113\n","correct count so far 36\n","114\n","115\n","correct count so far 37\n","116\n","117\n","118\n","correct count so far 38\n","119\n","correct count so far 39\n","120\n","correct count so far 40\n","121\n","122\n","123\n","correct count so far 41\n","124\n","125\n","correct count so far 42\n","126\n","127\n","correct count so far 43\n","128\n","correct count so far 44\n","129\n","130\n","correct count so far 45\n","131\n","132\n","correct count so far 46\n","133\n","correct count so far 47\n","134\n","135\n","136\n","137\n","correct count so far 48\n","138\n","correct count so far 49\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n"]},{"output_type":"execute_result","data":{"text/plain":["0.3356164383561644"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["33.6% accuracy on half model. 63.7% accuracy on full model."],"metadata":{"id":"7TTUSpT5mYdt"}},{"cell_type":"markdown","source":["--------------------------------------------------"],"metadata":{"id":"4T-0Apoca5oo"}}]}