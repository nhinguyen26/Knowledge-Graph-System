{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"convert_qa_train_to_natural_language.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98tKx1p0tma7","executionInfo":{"status":"ok","timestamp":1654374416546,"user_tz":420,"elapsed":32009,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"c46a91c9-3e57-48be-8b43-85f8823ea9fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","source":["%cd gdrive/MyDrive/24.7ai Project/Data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtVhsnV4ubUu","executionInfo":{"status":"ok","timestamp":1654374420425,"user_tz":420,"elapsed":501,"user":{"displayName":"Michelle Fong","userId":"07177557928124802487"}},"outputId":"d1dad19d-41a8-4cf3-da91-48fa3d719dff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/1ufxCi8Sp-EWkSn5IcUhOV-zTDoVvzGZe/24.7ai Project/Data\n"]}]},{"cell_type":"code","source":["mappings = dict()"],"metadata":{"id":"W51kAT7Quk4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import re\n","\n","# We create a set of entities that we come across in the files \n","# so that we know which ones we have data for\n","def get_seen_entities(lines):\n","  seen_entities = set()\n","  for line in lines:\n","    split_line = line.split('\\t')\n","    head = split_line[0]\n","    tail = split_line[-1]\n","    seen_entities.add(head)\n","    seen_entities.add(tail)\n","  return seen_entities\n","\n","# Using the entities that we've seen, we create a natural language\n","# version of the files if we have mappings for the entities within each line\n","def convert_to_nl(text, seen_entities, new_name):\n","  for raw_line in text:\n","    head = raw_line.split('[', 1)[1].split(']')[0]\n","    split_line = raw_line.split('\\t')\n","    split_line[-1] = split_line[-1].split('|')\n","    tails = split_line[-1]\n","    if len(tails) == 1:\n","      tails[0] = tails[0].replace('\\n', '')\n","    else:\n","      tails[-1] = tails[-1].replace('\\n', '')\n","\n","    count = 0\n","\n","    valid_tails = []\n","    for i in tails:\n","      if i in mappings:\n","        i = i.replace(i, mappings[i])\n","        tails[count] = i\n","        valid_tails.append(i)\n","        count += 1\n","  \n","    valid_head = False\n","    if head in mappings:\n","      split_line[0] = split_line[0].replace(head, mappings[head])\n","      valid_head = True\n","\n","    if len(valid_tails) > 0 and valid_head:\n","      if mappings[head] in seen_entities and not set(tails).isdisjoint(seen_entities):\n","        with open(new_name, 'a') as file:\n","          cleaned_tails = str(valid_tails).replace(\", \",\"|\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\")\n","          file.write(split_line[0] + '\\t' + cleaned_tails + '\\n')"],"metadata":{"id":"ZTmUCUNZEkoI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["iter = 0\n","\n","# We update our dictionary with the mapping between mid's \n","# and their natural language names\n","with open(\"mid2name.tsv\") as tsv:\n","    for line in csv.reader(tsv, dialect=\"excel-tab\"):\n","      new_entity = line[0].lstrip('/').replace('/', '.')\n","      mappings[new_entity] = line[1]\n","\n","with open('fb_natural_language_train.txt') as f:\n","    train_lines = f.readlines()\n","\n","with open('fb_natural_language_test.txt') as f:\n","    test_lines = f.readlines()\n","\n","with open('qa_train_webqsp.txt') as f:\n","    qa_train_lines = f.readlines()\n","\n","with open('qa_test_webqsp.txt') as f:\n","    qa_test_lines = f.readlines()"],"metadata":{"id":"8Bynl56HupjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_seen_entities = get_seen_entities(train_lines)\n","test_seen_entities = get_seen_entities(test_lines)"],"metadata":{"id":"4x-8QI6YG2WS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convert_to_nl(qa_train_lines, train_seen_entities, \"full_qa_train_nl.txt\")"],"metadata":{"id":"iIPRgv_zcptO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convert_to_nl(qa_test_lines, test_seen_entities, \"full_qa_test_nl.txt\")"],"metadata":{"id":"_hsYgtGmPuvc"},"execution_count":null,"outputs":[]}]}